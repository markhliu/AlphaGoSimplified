{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "653089c9",
   "metadata": {},
   "source": [
    "# Chapter 6: Alpha-Beta Pruning\n",
    "\n",
    "As you can see from Chapter 5, the minimax algorithm takes too long to make a move even with depth pruning. In this chapter, you'll use alpha beta pruning to save time by not searching certain branches that cannot possibly influence the final game outcome. \n",
    "\n",
    "To implement alpha-beta pruning, we keep track of two numbers: alpha and beta, the best outcome so far for players 1 and 2, respectively. Whenever we have alpha>-beta, or equivalently beta>-alpha, the minimax algorithm stop search a branch. \n",
    "\n",
    "We implement alpha-beta pruning in both Tic Tac Toe and Connect Four in this chapter. We show that the outcomes are the same with and without alpha-beta pruning. We also show that alpha-beta pruning saves significant amount of time for the player to find optimal moves. For example, in Tic Tac Toe, the amount of time for the minimax agent to come up with the first move decreases from 29 seconds without alpha-beta pruning to 1.6 seconds with alpha-beta pruning, a 94% reduction in the amount of time the minimax agent needs to come up with a move. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b25bf6",
   "metadata": {},
   "source": [
    "***\n",
    "$\\mathbf{\\text{Create a subfolder for files in Chapter 6}}$<br>\n",
    "***\n",
    "We'll put all files in Chapter 6 in a subfolder /files/ch06. Run the code in the cell below to create the subfolder.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "117477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"files/ch06\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ad876",
   "metadata": {},
   "source": [
    "## 1. What is Alpha Beta Pruning?"
   ]
  },
  {
   "attachments": {
    "node0.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "de2d7767",
   "metadata": {},
   "source": [
    "Let's first implement alpha-beta pruning in Tic Tac Toe and understand how it works. \n",
    "\n",
    "We use alpha to record the best outcome so far for Player X, and beta the best outcome for Player O. \n",
    "\n",
    "To demonstrate the idea behind alpha-beta pruning, let's assume the board in a Tic Tac Toe game is as follows: \n",
    "\n",
    "![node0.png](attachment:node0.png)\n",
    "\n",
    "Both Player X and Player O have made three moves. Now it's Player X's turn. First, assume that Player X uses minimax tree search to find the best next move. Next, we'll count how many branches Player X has to search. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db31da9",
   "metadata": {},
   "source": [
    "## 1.1. Minimax Tree Search without Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7252a608",
   "metadata": {},
   "source": [
    "Without pruning, Player X will search over six different scenarios:\n",
    "* Player X occupy 2; O occupy 3; X occupy 9; payoff to X is 0 and payoff to O is 0;\n",
    "* Player X occupy 2; O occupy 9; X occupy 3; payoff to X is 1 and payoff to O is -1;\n",
    "* Player X occupy 3; O occupy 2; payoff to X is -1 and payoff to O is 1;\n",
    "* Player X occupy 3; O occupy 9; X occupy 2; payoff to X is 1 and payoff to O is -1;\n",
    "* Player X occupy 9; O occupy 2; payoff to X is -1 and payoff to O is 1;\n",
    "* Player X occupy 9; O occupy 3; X occupy 2; payoff to X is 0 and payoff to O is 0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddc025",
   "metadata": {},
   "source": [
    "Since Player X knows that Player O chooses moves to maximize Player O's payoff, Player X anticipates the following outcomes:\n",
    "* Player X occupy 2; O occupy 3; payoff to X is 0 and payoff to O is 0;\n",
    "* Player X occupy 3; O occupy 2; payoff to X is -1 and payoff to O is 1;\n",
    "* Player X occupy 9; O occupy 2; payoff to X is -1 and payoff to O is 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05110e83",
   "metadata": {},
   "source": [
    "Therefore, Player X knows that her payoffs would be 0, -1 and -1, respectively if she were to choose cells 2, 3, and 9 in the next step. As a result, Player X will choose cell 2 in the next step. \n",
    "\n",
    "To summarize, without pruning, Player X needs to search for six different scenarios to come up with the best next move, which is cell 2. \n",
    "\n",
    "Next, we'll demonstrate that Player X can actually reach the same conclusion by searching over only four different scenarios by using alpha-beta pruning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee28a4e",
   "metadata": {},
   "source": [
    "## 1.2. Minimax Tree Search with Alpha-Beta Pruning\n",
    "With alpha-beta pruning, we can exclude certain branches of the search tree to save time. This, in turn, makes the tree search algorithm more powerful because it allows the AI players to search more branches in a fixed amount of time and come up with better strategies. \n",
    "\n",
    "We use alpha and beta to denote the best outcomes so far for the first player and the second player, respectively. At any point of the search process, if alpha>-beta (or equivalently, beta>-alpha), we stop searching the branch. \n",
    "\n",
    "Now, let's go back to the example in the last subsection where Player X needs to decide to place a game piece in cell 2, 3, or 9. With alpha-beta pruning, we can reduce the number of searches. \n",
    "\n",
    "Before start searching, let's set alpha and beta to -2. Since the worst outcome for either player is -1 (i.e., losing the game), setting the initial value to -2 guarantee that we keep accurate record of the two players' best payoffs so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfdfde6",
   "metadata": {},
   "source": [
    "After searching over the first two paths\n",
    "* Player X occupy 2; O occupy 3; X occupy 9; payoff to X is 0 and payoff to O is 0;\n",
    "* Player X occupy 2; O occupy 9; X occupy 3; payoff to X is 1 and payoff to O is -1;\n",
    "\n",
    "Player X knows that the outcome from placing a piece in cell 2 is a tie. So we record alpha=0 and beta=0. \n",
    "\n",
    "Next, player X starts to search on what happens if she places a piece in cell 3 instead. If she were to do that, Player O can place a piece in cell 2 and win. Therefore, after searching over the third path\n",
    "* Player X occupy 3; O occupy 2; payoff to X is -1 and payoff to O is 1;\n",
    "\n",
    "we update the value of beta to 1. At this point, since alpha>-beta (alpha is 1 and beta is 0), we don't need to spend time on the fourth scenario. \n",
    "\n",
    "Similarly, when player X starts to search on what happens if she places a piece in cell 9 instead, after searching over the fifth path\n",
    "* Player X occupy 9; O occupy 2; payoff to X is -1 and payoff to O is 1;\n",
    "\n",
    "we update the value of beta to 1. At this point, since beta>-alpha (or equivalently, alpha>-beta), we don't need to spend time on the sixth scenario. \n",
    "\n",
    "In summary, without alpha-beta pruning, Player X only needs to search for four different scenarios to come up with the best next move, which is also cell 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ebd81",
   "metadata": {},
   "source": [
    "## 1.3. Math behind the Alpha Beta Pruning\n",
    "If we use the payoff to the first player to denote the game outcome, then the first palyer tries to maximize the game outcome while the second palyer tries to minimize the game outcome. Hence the name minimax algorithm.\n",
    "\n",
    "In the example above, the game outcome as denoted by the payoff to the first player (player X) are as follows:\n",
    "* Player X occupy 2; O occupy 3; X occupy 9; game outcome is 0;\n",
    "* Player X occupy 2; O occupy 9; X occupy 3; game outcome is 1;\n",
    "* Player X occupy 3; O occupy 2; game outcome is -1;\n",
    "* Player X occupy 3; O occupy 9; X occupy 2; game outcome is 1;\n",
    "* Player X occupy 9; O occupy 2; game outcome is -1;\n",
    "* Player X occupy 9; O occupy 3; X occupy 2; game outcome is 0;\n",
    "\n",
    "Therefore, the minimax algorithm comes up with the following solution:\n",
    "\n",
    "`Game outcome = max{min{0,1},min{-1,1},min{-1,0}}`\n",
    "\n",
    "which leads to \n",
    "\n",
    "`Game outcome = max{0,-1,-1}=0`\n",
    "\n",
    "If we use the alpha-beta pruning, we can ignore path 4 and path 6, which leads to \n",
    "\n",
    "`Game outcome = max{min{0,1},min{-1,path4_outcome},min{-1,path6_outcome}}`\n",
    "\n",
    "Since min{-1,?} cannot be greater than -1, while min{0,1}=0. The game outcome will be 0 no matter what the outcomes from path 4 and path 6 are. That is, we know for sure that \n",
    "\n",
    "`Game outcome = max{min{0,1},min{-1,path4_outcome},min{-1,path6_outcome}}=0`\n",
    "\n",
    "without knowing the exact values of path4_outcome and path6_outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe6000",
   "metadata": {},
   "source": [
    "# 2. Alpha-Beta Pruning in Tic Tac Toe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1417875",
   "metadata": {},
   "source": [
    "In Chapter 5, you have seen that it took the minimax agent 29 seconds to make the first move in Tic Tac Toe. In this section, you'll learn to create a minimax agent with alpha-beta pruning and achieve the same results but less time. \n",
    "\n",
    "We'll modify the functions we used in Chapter 5 for the Tic Tac Toe minimax agent to incorporate the idea of alpha-beta pruning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7710c0ce",
   "metadata": {},
   "source": [
    "## 2.1. The maximized_payoff_ttt() Function\n",
    "We'll define a maximized_payoff_ttt() function. The function is similar to the maximized_payoff() function we defined in the Chapter 5. However, there are two important differences. First, the function keeps track of the best outcomes so far for players X and O and call them alpha and beta, respectively. Second, whenever the condition alpha>-beta (or equivalently beta>-alpha) is met, the algorithm stops searching the current branch.  \n",
    "\n",
    "The maximized_payoff_ttt() function is defined as follows. It's saved in the file ch06util.py, which you can download from the book's GitHub repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b94f675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximized_payoff_ttt(env,reward,done,alpha,beta):\n",
    "    # if the game has ended after the previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    if alpha==None:\n",
    "        alpha=-2\n",
    "    if beta==None:\n",
    "        beta=-2\n",
    "    if env.turn==\"X\":\n",
    "        best_payoff = alpha\n",
    "    if env.turn==\"O\":\n",
    "        best_payoff = beta         \n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=maximized_payoff_ttt(env_copy,\\\n",
    "                                     reward,done,alpha,beta)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff > best_payoff:        \n",
    "            best_payoff = my_payoff\n",
    "            if env.turn==\"X\":\n",
    "                alpha=best_payoff\n",
    "            if env.turn==\"O\":\n",
    "                beta=best_payoff \n",
    "        # skip the rest of the branch        \n",
    "        if alpha>=-beta:\n",
    "            break        \n",
    "    return best_payoff         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b208ef",
   "metadata": {},
   "source": [
    "Note this function applies to both Player X and Player O. The main difference here is that whenever the condition alpha>=-beta or beta>=-alpha is met, the player stop searching the rest of the branch when the hypothetical move m is taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6cead4",
   "metadata": {},
   "source": [
    "## 2.2. The minimax_ttt() Function\n",
    "We also define a minimax_ttt() function to produce the best move for the minimax agent. Instead of using one function for player X and one for player O, we crate the minimax_ttt() function for both players. \n",
    "\n",
    "The minimax_ttt() function is defined as follows. It's saved in the file ch06util.py that you just downloaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ef98d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_ttt(env):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward!=0:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=maximized_payoff_ttt(env_copy,\\\n",
    "                                     reward,done,-2,-2)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return choice(losses)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842bfefc",
   "metadata": {},
   "source": [
    "The minimax_ttt() function has just one argument: env, which is the game environment. It's similar to the minimax() function we defined in Chapter 5, with the exception that when a agent tries to figure out what's the opponent's payoff, the function maximized_payoff_ttt() is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838d4e3",
   "metadata": {},
   "source": [
    "## 2.3. Time Saved by Alpha-Beta Pruning\n",
    "Next, we test how fast is the depth-pruned minimax agent. We let the agent play first again and measure how long it takes for the agent to make a move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20703e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player X has chosen action=7\n",
      "It took the agent 1.6124515533447266 seconds\n",
      "the current state is \n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]]\n",
      "Player O, what's your move?\n",
      "6\n",
      "Player O has chosen action=6\n",
      "the current state is \n",
      "[[ 0  0  0]\n",
      " [ 0  0 -1]\n",
      " [ 1  0  0]]\n",
      "Player X has chosen action=5\n",
      "It took the agent 0.08851099014282227 seconds\n",
      "the current state is \n",
      "[[ 0  0  0]\n",
      " [ 0  1 -1]\n",
      " [ 1  0  0]]\n",
      "Player O, what's your move?\n",
      "3\n",
      "Player O has chosen action=3\n",
      "the current state is \n",
      "[[ 0  0 -1]\n",
      " [ 0  1 -1]\n",
      " [ 1  0  0]]\n",
      "Player X has chosen action=9\n",
      "It took the agent 0.006987094879150391 seconds\n",
      "the current state is \n",
      "[[ 0  0 -1]\n",
      " [ 0  1 -1]\n",
      " [ 1  0  1]]\n",
      "Player O, what's your move?\n",
      "1\n",
      "Player O has chosen action=1\n",
      "the current state is \n",
      "[[-1  0 -1]\n",
      " [ 0  1 -1]\n",
      " [ 1  0  1]]\n",
      "Player X has chosen action=8\n",
      "It took the agent 0.0009725093841552734 seconds\n",
      "the current state is \n",
      "[[-1  0 -1]\n",
      " [ 0  1 -1]\n",
      " [ 1  1  1]]\n",
      "Player X has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ch06util import minimax_ttt\n",
    "from utils.ttt_simple_env import ttt\n",
    "import time\n",
    "\n",
    "# Initiate the game environment\n",
    "env=ttt()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action = minimax_ttt(env)\n",
    "    end=time.time()\n",
    "    print(f\"Player X has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"Player X has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action = input(\"Player O, what's your move?\\n\")\n",
    "    print(f\"Player O has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"the current state is \\n{state.reshape(3,3)}\")\n",
    "    if done:\n",
    "        print(f\"Player O has won!\") \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a7d1a",
   "metadata": {},
   "source": [
    "It took only 1.6 seconds for the minimax agent to make the first move, instead of 29 seconds when alpha-beta pruning is not used. That's a huge improvement on the efficiency of the algorithm without affecting the effectiveness of the agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df6cd7",
   "metadata": {},
   "source": [
    "# 3. Test Minimax with Alpha-Beta Pruning\n",
    "Next, weâ€™ll make sure that the minimax algorith with alpha-beta pruning is as powerful as the minimax algorithm without pruning.  \n",
    "\n",
    "Below, we play ten games between the two algorithms. We let the minimax agent with alpha-beta pruning play first in five games and play second in the other five, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c3d78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import minimax_X,minimax_O,test_ttt_game \n",
    "from utils.ch06util import minimax_ttt \n",
    "\n",
    "results=[]\n",
    "for i in range(10):\n",
    "    # minimax with pruning moves first if i is an even number\n",
    "    if i%2==0:\n",
    "        result=test_ttt_game(env,minimax_ttt,minimax_O)\n",
    "        # record game outcome\n",
    "        results.append(result)\n",
    "    # minimax with pruning moves second if i is an odd number\n",
    "    else:\n",
    "        result=test_ttt_game(env,minimax_X,minimax_ttt)\n",
    "        # record negative of game outcome\n",
    "        results.append(-result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fa1bb",
   "metadata": {},
   "source": [
    "We now count how many times the minimax agent with pruning has won and lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787481ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimax agent has won 0 games\n",
      "the minimax agent has lost 0 games\n",
      "the game has tied 10 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times minimax with pruning won\n",
    "wins=results.count(1)\n",
    "print(f\"the minimax agent has won {wins} games\")\n",
    "# count how many times minimax with pruning lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the minimax agent has lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game has tied {ties} times\")          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907880a",
   "metadata": {},
   "source": [
    "The above results show that all ten games are tied. This indicates that the minimax agent with alpha-beta pruning is as effective as the minimax agent without pruning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a3674b",
   "metadata": {},
   "source": [
    "# 4. Alpha-Beta Pruning in Connect Four\n",
    "In Connect Four, even with alpha-beta pruning, it still takes forever for the minimax agent to make a move if we don't using depth pruning. We therefore combine alpha-beta pruning with depth pruning to make the game faster. \n",
    "\n",
    "As in Chapter 3, we'll keep the depth to 3 so that we can compare the speed with and without alpha-beta pruning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daff7f0",
   "metadata": {},
   "source": [
    "## 4.1. Add Alpha-Beta Pruning in Connect Four\n",
    "We first create a couple of functions in the local module ch06util."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eecbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_payoff_conn(env,reward,done,depth,alpha,beta):\n",
    "    # if the game has ended after the previous player's move\n",
    "    if done:\n",
    "        # if it's not a tie\n",
    "        if reward!=0:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    # If the maximum depth is reached, assume tie game\n",
    "    if depth==0:\n",
    "        return 0    \n",
    "    if alpha==None:\n",
    "        alpha=-2\n",
    "    if beta==None:\n",
    "        beta=-2\n",
    "    if env.turn==\"red\":\n",
    "        best_payoff = alpha\n",
    "    if env.turn==\"yellow\":\n",
    "        best_payoff = beta         \n",
    "    # iterate through all possible moves\n",
    "    for m in env.validinputs:\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m)  \n",
    "        # If I make this move, what's the opponent's response?\n",
    "        opponent_payoff=max_payoff_conn(env_copy,\\\n",
    "                                reward,done,depth-1,alpha,beta)\n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff > best_payoff:        \n",
    "            best_payoff = my_payoff\n",
    "            if env.turn==\"red\":\n",
    "                alpha=best_payoff\n",
    "            if env.turn==\"yellow\":\n",
    "                beta=best_payoff   \n",
    "        # Skip the rest of the branch\n",
    "        if alpha>=-beta:\n",
    "            break        \n",
    "    return best_payoff        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7af50",
   "metadata": {},
   "source": [
    "This function uses both depth pruning and alpha-beta pruning. It's similar to the max_payoff() function we defined in Chapter 5, but added in alpha-beta pruning. You can see the above function in the file ch06util.py as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3dac14",
   "metadata": {},
   "source": [
    "Another function we define is minimax_conn(), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f1d6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimax_conn(env,depth=3):\n",
    "    wins=[]\n",
    "    ties=[]\n",
    "    losses=[]  \n",
    "    # iterate through all possible next moves\n",
    "    for m in env.validinputs:\n",
    "        # make a hypothetical move and see what happens\n",
    "        env_copy=deepcopy(env)\n",
    "        state,reward,done,info=env_copy.step(m) \n",
    "        # If player X wins right away with move m, take it.\n",
    "        if done and reward!=0:\n",
    "            return m \n",
    "        # See what's the best response from the opponent\n",
    "        opponent_payoff=max_payoff_conn(env_copy,\\\n",
    "                            reward,done,depth,-2,-2)  \n",
    "        # Opponent's payoff is the opposite of your payoff\n",
    "        my_payoff=-opponent_payoff \n",
    "        if my_payoff==1:\n",
    "            wins.append(m)\n",
    "        elif my_payoff==0:\n",
    "            ties.append(m)\n",
    "        else:\n",
    "            losses.append(m)\n",
    "    # pick winning moves if there is any        \n",
    "    if len(wins)>0:\n",
    "        return choice(wins)\n",
    "    # otherwise pick tying moves\n",
    "    elif len(ties)>0:\n",
    "        return choice(ties)\n",
    "    return choice(losses)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52de1100",
   "metadata": {},
   "source": [
    "It's similar to the function minimax() we defined in Chapter 5. Here we added in alpha-beta pruning in addition to the depth pruning. The default depth is 3, but you have the option to change it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf67f75",
   "metadata": {},
   "source": [
    "## 4.2. Time Saved\n",
    "Next, we play a game with the Connect Four minimax agent with both depth pruning and alpha-beta pruning. We use the default depth of 3, and play a game with the agent. We let the agent play first again and measure how long it takes for the agent to make a move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0982762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The red player has chosen action=2\n",
      "It took the agent 0.015507221221923828 seconds\n",
      "the current state is \n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]]\n",
      "Player yellow, what's your move?\n",
      "7\n",
      "Player yellow has chosen action=7\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  1  0  0  0  0 -1]]\n",
      "The red player has chosen action=1\n",
      "It took the agent 0.029918909072875977 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  1  0  0  0  0 -1]]\n",
      "Player yellow, what's your move?\n",
      "7\n",
      "Player yellow has chosen action=7\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 1  1  0  0  0  0 -1]]\n",
      "The red player has chosen action=3\n",
      "It took the agent 0.04391336441040039 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 1  1  1  0  0  0 -1]]\n",
      "Player yellow, what's your move?\n",
      "7\n",
      "Player yellow has chosen action=7\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 1  1  1  0  0  0 -1]]\n",
      "The red player has chosen action=4\n",
      "It took the agent 0.03394508361816406 seconds\n",
      "the current state is \n",
      "[[ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 0  0  0  0  0  0 -1]\n",
      " [ 1  1  1  1  0  0 -1]]\n",
      "The red player has won!\n"
     ]
    }
   ],
   "source": [
    "from utils.ch06util import minimax_conn\n",
    "from utils.conn_env import conn\n",
    "import time\n",
    "\n",
    "# Initiate the game environment\n",
    "env=conn()\n",
    "state=env.reset()   \n",
    "# Play a full game manually\n",
    "while True:\n",
    "    # Mesure how long it takes to come up with a move\n",
    "    start=time.time()\n",
    "    action = minimax_conn(env,depth=3)\n",
    "    end=time.time()\n",
    "    print(f\"The red player has chosen action={action}\") \n",
    "    print(f\"It took the agent {end-start} seconds\")     \n",
    "    state, reward, done, info = env.step(action)\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==1:\n",
    "            print(f\"The red player has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break   \n",
    "    action = input(\"Player yellow, what's your move?\\n\")\n",
    "    print(f\"Player yellow has chosen action={action}\")    \n",
    "    state, reward, done, info = env.step(int(action))\n",
    "    print(f\"the current state is \\n{state.T[::-1]}\")\n",
    "    if done:\n",
    "        if reward==-1:\n",
    "            print(f\"The yellow player has won!\")  \n",
    "        else:\n",
    "            print(\"Game over, it's a tie!\")\n",
    "        break                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f218263f",
   "metadata": {},
   "source": [
    "The above results show that it took about 0.015 seconds for the agent to play a move. This is about 10% of the time the agent spent in Chapter 5 when only depth pruning is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f64596e",
   "metadata": {},
   "source": [
    "## 4.3. Test Effectivenes \n",
    "We'll test if the minimax algorithm with alpha-beta pruning is as effective as the one without alpha-beta pruning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97c5efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ch05util import minimax,test_conn_game \n",
    "from utils.ch06util import minimax_conn \n",
    "\n",
    "results=[]\n",
    "for i in range(100):\n",
    "    # minimax with pruning moves first if i is an even number\n",
    "    if i%2==0:\n",
    "        result=test_conn_game(env,minimax_conn,minimax)\n",
    "        # record game outcome\n",
    "        results.append(result)\n",
    "    # minimax with pruning moves second if i is an odd number\n",
    "    else:\n",
    "        result=test_conn_game(env,minimax,minimax_conn)\n",
    "        # record negative of game outcome\n",
    "        results.append(-result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0f31b",
   "metadata": {},
   "source": [
    "We create a list *results* to store game outcomes. We simulate 100 games and half the time, the minimax agent with alpha-beta pruning moves first and the other half, the minimax agent without alpha-beta pruning moves first. This way, no player has a first-mover advantage and we have a fair assessment of the effectiveness of the two algorithms. Whenever the minimax algorithm with alpha-beta pruning moves second, we multiple the outcome by -1 so that a value 1 in the list *results* indicates that the minimax with alpha-beta pruning has won. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d97135d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimax agent has won 35 games\n",
      "the minimax agent has lost 45 games\n",
      "the game has tied 20 times\n"
     ]
    }
   ],
   "source": [
    "# count how many times minimax with pruning won\n",
    "wins=results.count(1)\n",
    "print(f\"the minimax agent has won {wins} games\")\n",
    "# count how many times minimax with pruning lost\n",
    "losses=results.count(-1)\n",
    "print(f\"the minimax agent has lost {losses} games\")\n",
    "# count tie games\n",
    "ties=results.count(0)\n",
    "print(f\"the game has tied {ties} times\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095071fb",
   "metadata": {},
   "source": [
    "the minimax agent has won 47 games\n",
    "the minimax agent has lost 40 games\n",
    "the game has tied 13 times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec2255",
   "metadata": {},
   "source": [
    "The above results show that the minimax agent with alpha-beta pruning has won 47 times and lost 40 times. This shows that the mimimax agent with alpha-beta pruning is as effective as the agent without alpha-beta pruning. Note that since the outcomes are random, you may get results showing that the minimax agent with alpha-beta pruning has lost more often than it has won. It that happens, run the above two cells again and see what happens. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
